---
title: 'Nur ein wenig lineare Regression'
author: 'Norman Markgraf'
date: '2018-01-08'
slug: 'nur-ein-wenig-lineare-regression'
categories:  
    - Statistik
tags:
  - Lehre
  - R
  - Statistik
---



<p>Der <em>tipping</em> Datensatz wird oft analysiert. Das Verhältnis von Trinkgeld (<em>tip</em>) und Rechnungsbetrag (<em>total_bill</em>) steht dabei im Vordergrund einer lineare Regressionsanalyse. So auch hier. Wir wollen die einzelnen Angaben von <strong>R</strong> dabei in den Fokus rücken und einmal Hinterfragen, was wir bei der Ausgabe von <strong>R</strong> eigentlich genau sehen, woher es kommt und wie man es interprätieren kann.</p>
<p>Zunächt laden wir dazu die <strong>tipping</strong> Daten mittels</p>
<p>in den Arbeitsspeicher.</p>
<p>Eine lineares Modell wird schnell mit</p>
<pre class="r"><code>linMod &lt;- lm(tip ~ total_bill, data = tips)</code></pre>
<p>erstellt. Betrachten wir nun die Zusammenfassung:</p>
<pre class="r"><code>summary(linMod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.920270   0.159735   5.761 2.53e-08 ***
## total_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  0.4566, Adjusted R-squared:  0.4544 
## F-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Die zentrale Frage bei einer linearen Regression ist, finden wir einen linearen Zusammenhang in unserer Stichprobe, den wir auf die Population (als die Grundgesamtheit) übertragen können.</p>
<p>Die Spalte <strong>Estimate</strong> im Abschnitt <strong>Coefficients</strong> liefert uns in unser Stichprobe einen möglichen linearen Zusammenhang gemäß</p>
<p><span class="math display">\[\hat{y}_{\text{tip}} = \hat{\beta}_{\text{0}} + \hat{\beta}_{\text{total_bill}} \cdot x_{\text{total_bill}},\]</span></p>
<p>mit den <em>Regressionskoeffizienten</em> <span class="math inline">\(\hat{\beta}_0=0.9202696\)</span> und <span class="math inline">\(\hat{\beta}_{\text{total_bill}}=0.1050245\)</span>.</p>
<p>Graphisch ergibt sich damit das Modell wie folgt:</p>
<pre class="r"><code>plotModel(linMod)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Was hat es nun mit dem y-Achenabschnitt <span class="math inline">\(\hat{\beta}_0\)</span> aufsich?</p>
<p>Ist es etwa eine Art <em>Grundtrinkgeld</em>, mit dem der Kellern rechnen kann, auch wenn der Kunde gar nichts bestellt?</p>
<p>Nun ja, es so etwas in der Art, aber eben ein rein fiktiver Wert, der durch die Konstruktion der Parameter entsteht. Eine (affin-)lineare Gerade geht nun einmal irgendwann durch die y-Achse (wenn sie nicht parallel dazu ist) und es kann passieren, dass eine sinnvolle Interpretation nicht so ohne weiteres möglich ist.</p>
<p>Wir können aber dieses <em>Grundtrinkgeld</em> herraus nehmen und den y-Achenabschnitt auf Null setzen. Dazu ziehen wir <span class="math inline">\(\hat{\beta}_0\)</span> einfach von alle Trinkgeldern ab. Wir erhalten quasi nur noch den <em>Trinkgeldzuwach</em>.</p>
<pre class="r"><code>beta_0 &lt;- coef(linMod)[&quot;(Intercept)&quot;]  # Grundtrinkgeld
tips$delta_tip &lt;- tips$tip - beta_0    # wird abgezogen</code></pre>
<p>Vergleichen wir das alte lineare Modell mit dem neuen Modell (<em>linModDelta</em>):</p>
<pre class="r"><code>linModDelta &lt;- lm(delta_tip ~ total_bill, data = tips)
summary(linModDelta)</code></pre>
<pre><code>## 
## Call:
## lm(formula = delta_tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -4.549e-15  1.597e-01    0.00        1    
## total_bill   1.050e-01  7.365e-03   14.26   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  0.4566, Adjusted R-squared:  0.4544 
## F-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In diesem Modell ist der Wert für den y-Achenabschnitt numerisch gleich 0. – Ja, da mag zwar <span class="math inline">\(-4.5487837\times 10^{-15}\)</span> stehen, jedoch sind so kleine Werte der jedem Rechner inne wohnenden Ungenauigkeit in der Gleitkomma-Arithmetik geschuldet und wind faktisch gleich 0.</p>
<p>Der Wert für die Steigung lautet weiterhin <span class="math inline">\(0.1050245\)</span>. Das war auch zu erwarten, denn wir haben unsere Regressionsgerade eigentlich nur um <span class="math inline">\(\hat{\beta}_0\)</span> nach unten verschoben. (Der Fachmann spricht von einer Translation (Parallelverschiebung)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> um <span class="math inline">\(-\hat{\beta}_0\)</span>.</p>
<pre class="r"><code>plotModel(linModDelta)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Vergleichen wir die beiden Zusammenfassungen, so stellen wir fest das sich mit Aussnahme der <em>[Intercept]</em> Zeile parktisch nichts geändert hat. Das ist kein Wunder, sondern Absicht!</p>
<p>Die Regressionsgerade stellt für unsere Stichprobe die Gerade mit dem geringsten Fehler an den Datenpunkten dar. Mathematisch heißt das folgendes:</p>
<p>An den <span class="math inline">\(n=244\)</span> Datenpunkten unserer Stichprobe <span class="math inline">\((x_i, y_i)=(tips\$total\_bill[i], tips\$tip[i])\)</span> [für (i=1,,n)$] sind die <em>Residuen</em>, also die <em>Fehlerterme</em>, <span class="math display">\[
 \hat{e}_i =\hat{y}_i - y_i = \left[\hat{\beta}_{\text{0}} + \hat{\beta}_{\text{total_bill}} \cdot x_i\right] - y_i
\]</span></p>
<p>durch die verwendete <em>Methode der kleinsten Quadrate</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <em>quadratisch minimal</em>. Kurz:</p>
<p><span class="math display">\[
    \sum_{i=1}^n (\hat{e}_i)^2 \text{ ist minimal!}
\]</span></p>
<p>Wir können uns nun diese Fehlerterme auch graphisch Ansehen um die Varianz der Residuen zu sehen. Dazu siehen wir von allen Datenpunkt <span class="math inline">\(y_i\)</span> den geschätzen Wert <span class="math inline">\(\hat{y}_i\)</span> ab und erstellen ein neues lineares Modell</p>
<pre class="r"><code>beta_total_bill &lt;- coef(linModDelta)[&quot;total_bill&quot;]
tips$error_tip &lt;- (tips$tip - beta_0 - beta_total_bill * tips$total_bill)
linModError &lt;- lm(error_tip ~ total_bill, data = tips)
summary(linModError)</code></pre>
<pre><code>## 
## Call:
## lm(formula = error_tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  1.900e-15  1.597e-01       0        1
## total_bill  -8.740e-17  7.365e-03       0        1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  6.665e-31,  Adjusted R-squared:  -0.004132 
## F-statistic: 1.613e-28 on 1 and 242 DF,  p-value: 1</code></pre>
<pre class="r"><code>plotModel(linModError)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Wir können die Graphik im wesendlichen auch einfacher über den Befehl</p>
<pre class="r"><code>xyplot(residuals(linMod) ~ fitted(linMod))</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>erhalten.</p>
<p>Betrachten wir kurz nur die Rediduen:</p>
<pre class="r"><code>favstats(~residuals(linMod))</code></pre>
<pre><code>##        min         Q1      median        Q3      max          mean
##  -3.198225 -0.5651615 -0.09744499 0.4863111 3.743435 -2.022281e-17
##        sd   n missing
##  1.019943 244       0</code></pre>
<p>Wir sehe, dass wir in der Zusammenfassung immer genau diese Werte unter dem Abschnitt <em>Residuals</em> gefunden haben. Minimum, das 1. Quantil, der Median, das 3. Quantil und das Maximum stimmen überein.</p>
<p>Der erwartungstreue und unverzerrte Schätzer für den Standardfehler der Residuen, lautet</p>
<p><span class="math display">\[
\begin{align*}
    SE_{\text{Residuen}} &amp;= \sqrt{\frac{1}{n-2} \cdot \sum_{i=1}^n (\hat{e_i})^2} = \sqrt{\frac{n-1}{n-2} \cdot \frac{1}{n-1} \cdot \sum_{i=1}^n (\hat{e_i})^2} \\
                         &amp;= \sqrt{\frac{n-1}{n-2}} \cdot \sqrt{\frac{1}{n-1} \cdot \sum_{i=1}^n (\hat{e_i})^2} \\
                         &amp;= \sqrt{\frac{n-1}{n-2}} \cdot s_{\text{Residuen}}
  \end{align*}
\]</span></p>
<p>Also finden wir den Wert <em>Residual standard error</em> aus der Zeile</p>
<pre><code>Residual standard error: 1.022 on 242 degrees of freedom</code></pre>
<p>in dem wir den in den <em>favstats</em> gefundenen Wert für die Standardabweichung entsprechen korrigieren:</p>
<p><span class="math display">\[
SE_{\text{Residuen}} = \sqrt{\frac{n-1}{n-2}} \cdot s_{\text{Residuen}} = \sqrt{\frac{243}{242}} \cdot 1.0199426 = 1.0220477
\]</span></p>
<p>Der Median der Residuen ist nicht gleich Null, wie der Mittelwert. (Welcher auch hier als numerisch Null interpretiert werden muss!) Es könnte also eine linkssteile, rechtsschiefe Verteilung der Resiuden vorliegen. Betrachten wir dazu das Histogramm:</p>
<pre class="r"><code>histogram( ~residuals(linMod), nint = 19)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Schon beim ersten Blick auf das Histogramm kann an eine Normalverteilung der Residuen nicht mehr so ganz geglaubt werden.</p>
<p>Ein Shapiro-Wilk-Test<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> hat als Nullhypothese die Annahme, dass die Daten normalverteilt sind!</p>
<pre class="r"><code>shapiro.test( residuals(linMod) )</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(linMod)
## W = 0.96728, p-value = 2.171e-05</code></pre>
<p>Davon ist nach dem Ergebnis eben sowenig auszugehen, wie nach einem Blick auf das QQ-Normal-Diagram:</p>
<pre class="r"><code>qqnorm( residuals(linMod) )</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Ein K.O.-Kriterium für gute Prognosen.</p>
<p>Wie gut aber beschreibt unsere Regressionsgerade die Daten?</p>
<p>Als Maß dafür können wir das Bestimmtheitsmaß nehmen.</p>
<p>Ein kurzer Blick auf die Situation, der Mittelwert der Trinkgelder ist <span class="math display">\[
    \bar{y} =  \frac{1}{n} \sum_{i=1}^n y_i = 2.9982787.
\]</span></p>
<p>Wir erhalten so folgendes Diagramm:</p>
<pre class="r"><code>library(lattice)
mypanel &lt;- function(x, y) {
    panel.xyplot(x, y)
    panel.abline(h = mean(y), lwd=2, lty=2, col=&quot;darkgreen&quot;)
    panel.lmline(x, y, col=&quot;red&quot;, lwd=1, lty=2)
}
xyplot(
    tip ~ total_bill, data=tips, 
    panel = mypanel,
    main  = &quot;Streudiagramm der Trinkgelder&quot;,
    sub   = &quot;Grün der Mittelwert, rot die Regressionsgerade&quot;,
    ylab  = &quot;Trinkgeld&quot;,
    xlab  = &quot;Rechnungsbetrag&quot;
     )</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Die Varianz <span class="math inline">\(s^2_{y_i}=1.9144546\)</span> beschreibt die mittlere quadratische Abweichung der Datenpunkte <span class="math inline">\(y_i\)</span> vom Mittelwert <span class="math inline">\(\bar{y}\)</span>. Diese Varianz läßt sich nun Zerlegen in einen Anteil der durch die Regressionsgerade <em>erklärt</em> wird und in einen Anteil, der durch die Regressionsgerade nicht erklärt wird. <span class="math display">\[
    s^2_{y_i} = s^2_{\hat{y}_i} + s^2_{\hat{e}_i}
\]</span> Dividiert man beider Seiten durch die Varianz <span class="math inline">\(s^_{y_i}\)</span>, so normiert man den Ausdruck und kann den Faktor <span class="math inline">\(1/{n-1}\)</span> (bzw. <span class="math inline">\(1/n\)</span>) herauskürzen. Es bleibt dann:</p>
<p><span class="math display">\[
    1 = \frac{\sum_{i=1}^n (\bar{y}- \hat{y_i})^2}{\sum_{i=1}^n (\bar{y}-y_i)^2} + \frac{\sum_{i=1}^n (\hat{e_i})^2}{\sum_{i=1}^n (\bar{y}-y_i)^2}
\]</span></p>
<p>Multipliziert man nun beide Seiten mit <span class="math inline">\(\sum_{i=1}^n (y_i)^2\)</span>, so erhält man: <span class="math display">\[
    \sum_{i=1}^n (\bar{y}- y_i)^2 = \sum_{i=1}^n (\bar{y}- \hat{y_i})^2+ \sum_{i=1}^n (\hat{e_i})^2 
\]</span></p>
<p>Zur vereinfachung nennt man nun die einzelnen Summen in dem ausdruck wie folgt:</p>
<ul>
<li>Der erste Ausdruck heißt <strong>Gesamtvarianz* oder </strong>total sum of squares** oder kurz <strong><span class="math inline">\(SS_T\)</span></strong>, (oder <strong>TSS</strong>) er ist die Summe der quadrierten Differenzen <span class="math display">\[
  SS_T = \sum_{i=1}^n (\bar{y}-y_i)^2
\]</span></li>
</ul>
<p>– Der zweite Ausdruck heißt <strong>Modellvarianz</strong> oder <strong>model sum of squares</strong> oder kurz <strong><span class="math inline">\(SS_M\)</span></strong> (oder <strong>RSS</strong>), er ist die Summe der quadrierten Differenzen aus dem Mittelwert <span class="math inline">\(\bar{y}\)</span> und der Punkte auf der Regressionsgeraden <span class="math inline">\(\hat{y}_i\)</span>: <span class="math display">\[
    SS_M = \sum_{i=1}^n (\bar{y}-\hat{y}_i)^2
\]</span></p>
<ul>
<li>Der dritte Ausdruck heißt <strong>Gesamt-Verhersage-Fehler</strong>, <strong>Fehlersteuung der Regression</strong> oder <strong>error sum of squares</strong> oder kurz <span class="math inline">\(SS_E\)</span> (oder <strong>ESS</strong>), er ist die Summe der quadratischen Differenz aus den Datenpunkten <span class="math inline">\(y_i\)</span> und den Punkten der Regressionsgeraden <span class="math inline">\(\hat{y}_i\)</span>: <span class="math display">\[
  SS_E = \sum_{i=1}^n (\hat{y}_i-y_i)^2 = \sum_{i=1}^n (\hat{e}_i)^2
\]</span></li>
</ul>
<p>Wir können daher auch kurz <span class="math display">\[
    SS_T = SS_M + SS_E
\]</span> schreiben und sparen uns die ganzen Summenzeichen.</p>
<p>Die Güte einer Regression wollen wir nun durch den Anteil der durch das Model erklärten Varianz (also der <span class="math inline">\(SS_M\)</span>) ausdrücken und stellen daher nach <span class="math inline">\(SS_M\)</span> um: <span class="math display">\[
    SS_M = SS_T - SS_E
\]</span> Teilen wir beide Seiten durch <span class="math inline">\(SS_T\)</span> also der maximalen (weil totalen) Quadratsumme, so erhalten wir: <span class="math display">\[
    \frac{SS_M}{SS_T} = \frac{SS_T}{SS_T} - \frac{SS_E}{SS_T} = 1 - \frac{SS_E}{SS_T}
\]</span></p>
<p>Den Ausdruck <span class="math inline">\(\frac{SS_M}{SS_T}\)</span> nennen wir <strong>Bestimmtheitsmaß</strong> (<span class="math inline">\(R^2\)</span>). Es ist ein Wert zwischen 0 und 1, der den Anteil der durch das Modell beschriebenen Varianz in bezug auf die gesamt Varianz angibt.</p>
<p>In unserer Zusammenfassung des linearen Models findet sich dieser Wert auch. Und zwar unter dem Begriff:</p>
<pre><code>Multiple R-squared:  0.4566</code></pre>
<p>Es gilt ja:</p>
<p><span class="math display">\[
    R^2 = 1 - \frac{SS_E}{SS_T} = 1 - \frac{s^2_{\hat{e}_i}}{s^2_{y_i}} = 1 - \frac{1.0402829}{1.9144546} = 0.4566166
\]</span></p>
<p>Der Wert</p>
<pre><code>Adjusted R-squared:  0.4544</code></pre>
<p>erklärt sich nun daraus<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, dass das Bestimmheitsmaß um so größer wird, je größer die Zahl der unabhöngigen Variablen wird. Und zwar <em>unabhöngig</em> davon, ob weitere unabhängige Variablen wirklich einen Beitrag zur Erklärungskraft liefern. Daher nutzt man besser das <strong>korrigierte Bestimmtheitsmaß</strong> (engl: <em>adjusted R-squared</em>) <span class="math inline">\(\bar{R}^2\)</span>: <span class="math display">\[
    \bar{R}^2 = 1- (1-R^2) \cdot \frac{n-1}{n-p-1} = R^2 - (1-R^2)  \cdot \frac{p}{n-p-1}
\]</span></p>
<p>Wobei <span class="math inline">\(p\)</span> die Anzahl der unabhängigen Variablen im Modell darstellt. In unsereem Beispiel gilt daher:</p>
<p><span class="math display">\[
    \bar{R}^2 = 1 - (1-R^2)  \cdot \frac{n-1}{n-p-1} =  1 - (1- 0.4566166)  \cdot \frac{244-1}{244- 1- 1} = 0.4543712
\]</span></p>
<p><strong>Vorsicht:</strong> Das <em>korrigierte Bestimmtheitsmaß</em> ist nicht mehr an das Intervall <span class="math inline">\([0; 1]\)</span> gebunden! Es kann negative Werte annehmen, ist in der Regel kleiner als das (unkorrigierte) Bestimmtheitsmaß und erreicht die obere Grenze (<span class="math inline">\(\bar{R}^2=1\)</span>) genau dann, wenn <span class="math inline">\(R^2 = 1\)</span> ist.</p>
<p>Bei der <strong>Gesamtsignifikanz des Modells</strong> (auch <strong>Overall-F-Test</strong> genannt) wird geprüft, ob mindestens eine Variable einen Erklärungsgehalt für das Modell liefert.</p>
<p>Falls diese Hypothese verworfen wird ist somit das Modell nutzlos. Dieser Test lässt sich so interpretieren als würde man die gesamte Güte des Modells, also das <span class="math inline">\(R^2\)</span> des Modells, testen. Aus diesem Grund wird der F-Test der Gesamtsignifikanz des Modells auch als Anpassungsgüte-Test bezeichnet. Die Nullhypothese des F-Test der Gesamtsignifikanz des Modells sagt aus, dass alle erklärenden Variablen keinen Einfluss auf die abhängige Variable haben. Sowohl die abhängige Variable als auch die unabhängigen Variablen können binär (kategoriell) oder metrisch sein. Der <em>Wald-Test</em> kann dann die Hypothesen testen (ohne Einbezug des Achsenabschnittes): <span class="math display">\[
    H_{0}\colon \beta _{1}=\beta _{2}=\ldots =\beta _{k}\;=\;0\Rightarrow R^{2}=0
\]</span> gegen<br />
<span class="math display">\[
    H_{1}:\beta _{j}\;\neq \;0\;\mathrm {f{\ddot {u}}r\;mindestens\;ein} \;j\in \{1,\ldots ,k\}\Rightarrow R^{2}\neq 0
\]</span></p>
<p>Die Teststatistik dieses Tests lautet <span class="math display">\[
F(p,n−p){\displaystyle {\begin{aligned}F\;\;{\stackrel {H_{0}}{=}}{\frac {R^{2}}{1-R^{2}}}{\frac {n-p-1}{p}}\;\;{\stackrel {H_{0}}{\sim }}\;\;F(p,n-p)\end{aligned}}}.
\]</span></p>
<p>Die Teststatistik <span class="math inline">\(\displaystyle (n-p-1)\)</span> und <span class="math inline">\(p\)</span> Freiheitsgraden. Überschreitet der empirische F-Wert einen kritischen F-Wert, der zu einem a priori festgelegten Signifikanzniveau <span class="math inline">\(\alpha\)</span>, so verwirft man die Nullhypothese <span class="math inline">\(H_{0}\)</span>. Das <span class="math inline">\(R^{2}\)</span> ist dann ausreichend groß und mindestens ein Regressor trägt also vermutlich genügend viel Information zur Erklärung von <span class="math inline">\(y\)</span> bei. Es ist naheliegend bei hohen F-Werten die Nullhypothese zu verwerfen, da ein hohes Bestimmtheitsmaß zu einem hohen F-Wert führt. Wenn der <em>Wald-Test</em> für eine oder mehrere unabhängige Variablen die Nullhypothese ablehnt, dann kann man davon ausgehen, dass die zugehörigen Parameter ungleich Null sind, so dass die Variable(n) in das Modell mit einbezogen werden sollten.</p>
<p>In unserem Beispiel is <span class="math display">\[
    {\frac {R^{2}}{1-R^{2}}}{\frac {n-p-1}{p}} = \frac{0.4566166}{1-0.4566166} \cdot \frac{244-1-1}{1} = 203.3577233
\]</span></p>
<p>der Wert in der Zeile</p>
<pre><code>F-statistic: 203.4 on 1 and 242 DF</code></pre>
<p>mit Parametern <span class="math inline">\(p=1\)</span> und <span class="math inline">\(n-p-1=242\)</span> Freiheitsgraden.</p>
<p>Der p-Wert von (numerisch) 0, liefert also ein hinreichendes Indiz dafür, dass der Rechnungsbetrag einen echten Beitrag liefert.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>vgl: <a href="https://de.wikipedia.org/wiki/Parallelverschiebung" class="uri">https://de.wikipedia.org/wiki/Parallelverschiebung</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>vgl: <a href="https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate" class="uri">https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>vgl: <a href="https://de.wikipedia.org/wiki/Shapiro-Wilk-Test" class="uri">https://de.wikipedia.org/wiki/Shapiro-Wilk-Test</a><a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>vgl: <a href="https://de.wikipedia.org/wiki/Bestimmtheitsmaß#Das_korrigierte_Bestimmtheitsmaß" class="uri">https://de.wikipedia.org/wiki/Bestimmtheitsmaß#Das_korrigierte_Bestimmtheitsmaß</a><a href="#fnref4" class="footnote-back">↩</a></p></li>
</ol>
</section>
