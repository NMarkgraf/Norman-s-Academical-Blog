---
title: Nur ein wenig lineare Regression
author: Norman Markgraf
date: '2018-01-08'
slug: nur-ein-wenig-lineare-regression
categories:
  - Statistik
tags:
  - Lehre
  - R
  - Statistik
---



<p>Der <em>tipping</em> Datensatz wird oft analysiert. Das Verhältnis von Trinkgeld (<em>tip</em>) und Rechnungsbetrag (<em>total_bill</em>) steht dabei im Vordergrund einer lineare Regressionsanalyse. So auch hier. Wir wollen die einzelnen Angaben von <strong>R</strong> dabei in den Fokus rücken und einmal Hinterfragen, was wir bei der Ausgabe von <strong>R</strong> eigentlich genau sehen, woher es kommt und wie man es interprätieren kann.</p>
<p>Zunächt laden wir dazu die <strong>tipping</strong> Daten mittels</p>
<pre class="r"><code>library(mosaic)
download.file(&quot;https://goo.gl/whKjnl&quot;, destfile = &quot;tips.csv&quot;)
tips &lt;- read.csv2(&quot;tips.csv&quot;)</code></pre>
<p>in den Arbeitsspeicher.</p>
<p>Eine lineares Modell wird schnell mit</p>
<pre class="r"><code>linMod &lt;- lm(tip ~ total_bill, data = tips)</code></pre>
<p>erstellt. Betrachten wir nun die Zusammenfassung:</p>
<pre class="r"><code>summary(linMod)</code></pre>
<pre><code>## 
## Call:
## lm(formula = tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.920270   0.159735   5.761 2.53e-08 ***
## total_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  0.4566, Adjusted R-squared:  0.4544 
## F-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Die zentrale Frage bei einer linearen Regression ist, finden wir einen linearen Zusammenhang in unserer Stichprobe, den wir auf die Population (als die Grundgesamtheit) übertragen können.</p>
<p>Die Spalte <strong>Estimate</strong> im Abschnitt <strong>Coefficients</strong> liefert uns in unser Stichprobe einen möglichen linearen Zusammenhang gemäß</p>
<p><span class="math display">\[\hat{y}_{\text{tip}} = \hat{\beta}_{\text{0}} + \hat{\beta}_{\text{total_bill}} \cdot x_{\text{total_bill}},\]</span></p>
<p>mit den <em>Regressionskoeffizienten</em> <span class="math inline">\(\hat{\beta}_0=0.9202696\)</span> und <span class="math inline">\(\hat{\beta}_{\text{total_bill}}=0.1050245\)</span>.</p>
<p>Graphisch ergibt sich damit das Modell wie folgt:</p>
<pre class="r"><code>plotModel(linMod)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Was hat es nun mit dem y-Achenabschnitt <span class="math inline">\(\hat{\beta}_0\)</span> aufsich?</p>
<p>Ist es etwa eine Art <em>Grundtrinkgeld</em>, mit dem der Kellern rechnen kann, auch wenn der Kunde gar nichts bestellt?</p>
<p>Nun ja, es so etwas in der Art, aber eben ein rein fiktiver Wert, der durch die Konstruktion der Parameter entsteht. Eine (affin-)lineare Gerade geht nun einmal irgendwann durch die y-Achse (wenn sie nicht parallel dazu ist) und es kann passieren, dass eine sinnvolle Interpretation nicht so ohne weiteres möglich ist.</p>
<p>Wir können aber dieses <em>Grundtrinkgeld</em> herraus nehmen und den y-Achenabschnitt auf Null setzen. Dazu ziehen wir <span class="math inline">\(\hat{\beta}_0\)</span> einfach von alle Trinkgeldern ab. Wir erhalten quasi nur noch den <em>Trinkgeldzuwach</em>.</p>
<pre class="r"><code>beta_0 &lt;- coef(linMod)[&quot;(Intercept)&quot;]  # Grundtrinkgeld
tips$delta_tip &lt;- tips$tip - beta_0    # wird abgezogen</code></pre>
<p>Vergleichen wir das alte lineare Modell mit dem neuen Modell (<em>linModDelta</em>):</p>
<pre class="r"><code>linModDelta &lt;- lm(delta_tip ~ total_bill, data = tips)
summary(linModDelta)</code></pre>
<pre><code>## 
## Call:
## lm(formula = delta_tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -4.549e-15  1.597e-01    0.00        1    
## total_bill   1.050e-01  7.365e-03   14.26   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  0.4566, Adjusted R-squared:  0.4544 
## F-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In diesem Modell ist der Wert für den y-Achenabschnitt numerisch gleich 0. – Ja, da mag zwar <span class="math inline">\(-4.5487837\times 10^{-15}\)</span> stehen, jedoch sind so kleine Werte der jedem Rechner inne wohnenden Ungenauigkeit in der Gleitkomma-Arithmetik geschuldet und wind faktisch gleich 0.</p>
<p>Der Wert für die Steigung lautet weiterhin <span class="math inline">\(0.1050245\)</span>. Das war auch zu erwarten, denn wir haben unsere Regressionsgerade eigentlich nur um <span class="math inline">\(\hat{\beta}_0\)</span> nach unten verschoben. (Der Fachmann spricht von einer Translation (Parallelverschiebung)<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> um <span class="math inline">\(-\hat{\beta}_0\)</span>.</p>
<pre class="r"><code>plotModel(linModDelta)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Vergleichen wir die beiden Zusammenfassungen, so stellen wir fest das sich mit Aussnahme der <em>[Intercept]</em> Zeile parktisch nichts geändert hat. Das ist kein Wunder, sondern Absicht!</p>
<p>Die Regressionsgerade stellt für unsere Stichprobe die Gerade mit dem geringsten Fehler an den Datenpunkten dar. Mathematisch heißt das folgendes:</p>
<p>An den <span class="math inline">\(n=244\)</span> Datenpunkten unserer Stichprobe <span class="math inline">\((x_i, y_i)=(tips\$total\_bill[i], tips\$tip[i])\)</span> [für (i=1,,n)$] sind die <em>Residuen</em>, also die <em>Fehlerterme</em>, <span class="math display">\[
 \hat{e}_i =\hat{y}_i - y_i = \left[\hat{\beta}_{\text{0}} + \hat{\beta}_{\text{total_bill}} \cdot x_i\right] - y_i
\]</span></p>
<p>durch die verwendete <em>Methode der kleinsten Quadrate</em><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a> <em>quadratisch minimal</em>. Kurz:</p>
<p><span class="math display">\[
    \sum_{i=1}^n (\hat{e}_i)^2 \text{ ist minimal!}
\]</span></p>
<p>Wir können uns nun diese Fehlerterme auch graphisch Ansehen um die Varianz der Residuen zu sehen. Dazu siehen wir von allen Datenpunkt <span class="math inline">\(y_i\)</span> den geschätzen Wert <span class="math inline">\(\hat{y}_i\)</span> ab und erstellen ein neues lineares Modell</p>
<pre class="r"><code>beta_total_bill &lt;- coef(linModDelta)[&quot;total_bill&quot;]
tips$error_tip &lt;- (tips$tip - beta_0 - beta_total_bill * tips$total_bill)
linModError &lt;- lm(error_tip ~ total_bill, data = tips)
summary(linModError)</code></pre>
<pre><code>## 
## Call:
## lm(formula = error_tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  1.900e-15  1.597e-01       0        1
## total_bill  -8.740e-17  7.365e-03       0        1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  6.665e-31,  Adjusted R-squared:  -0.004132 
## F-statistic: 1.613e-28 on 1 and 242 DF,  p-value: 1</code></pre>
<pre class="r"><code>plotModel(linModError)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Wir können die Graphik im wesendlichen auch einfacher über den Befehl</p>
<pre class="r"><code>xyplot(residuals(linMod) ~ fitted(linMod))</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>erhalten.</p>
<p>Betrachten wir kurz nur die Rediduen:</p>
<pre class="r"><code>favstats(~residuals(linMod))</code></pre>
<pre><code>##        min         Q1      median        Q3      max          mean
##  -3.198225 -0.5651615 -0.09744499 0.4863111 3.743435 -2.022281e-17
##        sd   n missing
##  1.019943 244       0</code></pre>
<p>Wir sehe, dass wir in der Zusammenfassung immer genau diese Werte unter dem Abschnitt <em>Residuals</em> gefunden haben. Minimum, das 1. Quantil, der Median, das 3. Quantil und das Maximum stimmen überein.</p>
<p>Der erwartungstreue und unverzerrte Schätzer für den Standardfehler der Residuen, lautet</p>
<p><span class="math display">\[
\begin{align*}
    SE_{\text{Residuen}} &amp;= \sqrt{\frac{1}{n-2} \cdot \sum_{i=1}^n (\hat{e_i})^2} = \sqrt{\frac{n-1}{n-2} \cdot \frac{1}{n-1} \cdot \sum_{i=1}^n (\hat{e_i})^2} \\
                         &amp;= \sqrt{\frac{n-1}{n-2}} \cdot \sqrt{\frac{1}{n-1} \cdot \sum_{i=1}^n (\hat{e_i})^2} \\
                         &amp;= \sqrt{\frac{n-1}{n-2}} \cdot s_{\text{Residuen}}
  \end{align*}
\]</span></p>
<p>Also finden wir den Wert <em>Residual standard error</em> aus der Zeile</p>
<pre><code>Residual standard error: 1.022 on 242 degrees of freedom</code></pre>
<p>in dem wir den in den <em>favstats</em> gefundenen Wert für die Standardabweichung entsprechen korrigieren:</p>
<p><span class="math display">\[
SE_{\text{Residuen}} = \sqrt{\frac{n-1}{n-2}} \cdot s_{\text{Residuen}} = \sqrt{\frac{243}{242}} \cdot 1.0199426 = 1.0220477
\]</span></p>
<p>Der Median der Residuen ist nicht gleich Null, wie der Mittelwert. (Welcher auch hier als numerisch Null interpretiert werden muss!) Es könnte also eine linkssteile, rechtsschiefe Verteilung der Resiuden vorliegen. Betrachten wir dazu das Histogramm:</p>
<pre class="r"><code>histogram( ~residuals(linMod), nint=19)</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Schon beim ersten Blick auf das Histogramm kann an eine Normalverteilung der Residuen nicht mehr so ganz geglaubt werden.</p>
<p>Ein Shapiro-Wilk-Test<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> hat als Nullhypothese <span class="math inline">\(H_0\)</span> die Annahme, dass die Daten normalverteilt sind!</p>
<pre class="r"><code>shapiro.test( residuals(linMod) )</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(linMod)
## W = 0.96728, p-value = 2.171e-05</code></pre>
<p>Davon ist nach dem Ergebnis eben sowenig auszugehen, wie nach einem Blick auf das QQ-Normal-Diagram:</p>
<pre class="r"><code>qqnorm( residuals(linMod) )</code></pre>
<p><img src="/nab/post/2018-01-08-nur-ein-wenig-lineare-regression_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p>vgl: <a href="https://de.wikipedia.org/wiki/Parallelverschiebung" class="uri">https://de.wikipedia.org/wiki/Parallelverschiebung</a><a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>vgl: <a href="https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate" class="uri">https://de.wikipedia.org/wiki/Methode_der_kleinsten_Quadrate</a><a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>vgl: <a href="https://de.wikipedia.org/wiki/Shapiro-Wilk-Test" class="uri">https://de.wikipedia.org/wiki/Shapiro-Wilk-Test</a><a href="#fnref3" class="footnote-back">↩</a></p></li>
</ol>
</section>
