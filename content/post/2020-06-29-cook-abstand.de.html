---
title: Cook Abstand
author: Norman Markgraf
date: '2020-06-29'
slug: cook-abstand
categories:
  - Statistisches
tags:
  - Cook Abstand
  - Cooks Distance
  - Ausreißer
  - Outliers
subtitle: ''
summary: ''
authors: []
lastmod: '2020-06-29T10:52:38+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>Eine Möglichkeit Ausreißer zu finden ist der <em>Cook Abstand</em>
(eng.: <em>Cook’s distance</em>).
Die Idee ist es zu messen welchen Einfluss ein Wert auf das Modell hat.
Dazu schaut mich sich das Modell einmal mit und einmal ohne den Wert an und
vergleicht diese Ergebnisse.</p>
<p>Schauen wir uns den Cook Abstand einmal für ein (einfaches) lineares
Regressionmodell konkret an:</p>
<div id="vorbereitungen" class="section level3">
<h3>Vorbereitungen</h3>
<p>Wir wollen mit <em>mosaic</em> arbeiten, also laden wir das Paket als erstes:</p>
<pre class="r"><code>library(mosaic)</code></pre>
<p>Falls die <strong>tipping</strong>-Daten noch nicht im Verzeichnis liegen,
laden wir diese aus dem Internet nach:</p>
<pre class="r"><code>if (!file.exists(&quot;tips.csv&quot;)) {
  download.file(&quot;https://goo.gl/whKjnl&quot;, destfile = &quot;tips.csv&quot;)
}</code></pre>
<p>Nun laden wir die <strong>tipping</strong>-Daten in den Speicher in den Datenrahmen <code>tips</code>:</p>
<pre class="r"><code>tips &lt;- read.csv2(&quot;tips.csv&quot;)</code></pre>
<p>Wir brauchen für unser Modell nur den Rechnungsbetrag <code>total_bill</code> und den
Trinkgeldbetrag <code>tip</code> für unser Modell:</p>
<pre class="r"><code>tips %&gt;% select(c(&quot;total_bill&quot;, &quot;tip&quot;)) -&gt; tips</code></pre>
</div>
<div id="unser-modell" class="section level2">
<h2>Unser Modell:</h2>
<pre class="r"><code>gf_point(tip ~ total_bill, data = tips)</code></pre>
<p><img src="/post/2020-06-29-cook-abstand.de_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>erg_lm &lt;- lm(tip ~ total_bill, data = tips)
summary(erg_lm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = tip ~ total_bill, data = tips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.1982 -0.5652 -0.0974  0.4863  3.7434 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.920270   0.159735   5.761 2.53e-08 ***
## total_bill  0.105025   0.007365  14.260  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.022 on 242 degrees of freedom
## Multiple R-squared:  0.4566, Adjusted R-squared:  0.4544 
## F-statistic: 203.4 on 1 and 242 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Betrachten wir nun die Regressionsgerade in unseren Daten:</p>
<pre class="r"><code>gf_point(tip ~ total_bill, data = tips) +
  geom_line(aes(x = total_bill, 
                y = erg_lm$fitted.values, 
                color = &quot;Regressionsgerade&quot;))</code></pre>
<p><img src="/post/2020-06-29-cook-abstand.de_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Für lineare Regressionsmodell können einflussreiche Ausreißer sehr hinderlich
sein.
Was also ändert sich nun, wenn wir einen Wert, z.B. einen Ausreißer,
nicht betrachten?</p>
<p>Als Beispiel wählen wir die folgende Beobachtung aus:</p>
<pre class="r"><code>tips %&gt;% slice(173) -&gt; tips_removed
tips_removed</code></pre>
<pre><code>##   total_bill  tip
## 1       7.25 5.15</code></pre>
<pre class="r"><code>tips %&gt;% slice(-173) -&gt; tips_red
erg_lm_red &lt;- lm(tip ~ total_bill, data = tips_red)
summary(erg_lm_red)</code></pre>
<pre><code>## 
## Call:
## lm(formula = tip ~ total_bill, data = tips_red)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.2136 -0.5351 -0.0818  0.4951  3.6869 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.86065    0.15709   5.479 1.08e-07 ***
## total_bill   0.10731    0.00723  14.843  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9992 on 241 degrees of freedom
## Multiple R-squared:  0.4776, Adjusted R-squared:  0.4754 
## F-statistic: 220.3 on 1 and 241 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>gf_point(tip ~ total_bill, data = tips_red) +
  geom_line(aes(x = total_bill, 
                y = erg_lm_red$fitted.values, 
                color = &quot;Regressionsgerade&quot;)) +
  geom_point(aes(x = tips_removed$total_bill,
                 y = tips_removed$tip,
                 colour = &quot;Removed Data&quot;,
                 show.legend = FALSE))</code></pre>
<pre><code>## Warning: Ignoring unknown aesthetics: show.legend</code></pre>
<p><img src="/post/2020-06-29-cook-abstand.de_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Um zu messen, was diese Änderung bewirkt hat, schaut sich der Cook Abstand
zunächst die Summe der quadrierten Differenzen der vorhergesagten Werte in
beiden Modellen an:</p>
<pre class="r"><code>new_data &lt;- data_frame(total_bill = tips$total_bill)</code></pre>
<pre><code>## Warning: `data_frame()` is deprecated as of tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_warnings()` to see where this warning was generated.</code></pre>
<pre class="r"><code>prognose_lm &lt;- predict(erg_lm, newdata = new_data)
prognose_lm_red &lt;- predict(erg_lm_red, newdata = new_data)</code></pre>
<p><span class="math display">\[d_j = \sum_{i=1}^n \left(\hat{y}_i - \hat{y}_{i(j)}\right)^2\]</span>
Dabei ist <span class="math inline">\(\hat{y}_i\)</span> die Prognose des Wertes <span class="math inline">\(y_i\)</span> auf Basis von <span class="math inline">\(x_i\)</span> mit
dem Originalmodell und <span class="math inline">\(\hat{y}_{i(j)}\)</span> die Prognose wenn man die <span class="math inline">\(j\)</span>-te
Beobachtung aus dem Modell gestrichen hat.</p>
<pre class="r"><code>d_j &lt;- sum((prognose_lm - prognose_lm_red)^2)
d_j</code></pre>
<pre><code>## [1] 0.1511406</code></pre>
<p>Der Cook Abstand <span class="math inline">\(D_j\)</span> wird nun noch <em>normiert</em> durch
<span class="math display">\[var_{cook} = p \cdot s_{\epsilon_i^2}^2\]</span>
Dabei ist <span class="math inline">\(s_{\epsilon_i^2}^2\)</span> der erwartungstreue Schätzer der Varianz der
Residuen und <span class="math inline">\(p\)</span> die Anzahl aller erklärenden Variablen (hier <span class="math inline">\(1\)</span>) <span class="math inline">\(+ 1\)</span>.</p>
<p>Es ist also:</p>
<p><span class="math display">\[D_j = \frac{d_j}{var_{cook}} = \frac{\sum\limits_{i=1}^n \left(\hat{y}_i - \hat{y}_{i(j)}\right)^2}{p \cdot s_{\epsilon_i^2}^2}\]</span></p>
<pre class="r"><code># Summary des Modells
selm &lt;- summary(erg_lm)
# Wir finden p als rank im Modell
p &lt;- erg_lm$rank 
# Wir finden den erwatungtreuen Schätzer im Summary des Modells
s_quad_eps_quad &lt;- (selm$sigma)^2 

var_cook = p * s_quad_eps_quad

D_j = d_j / var_cook
D_j</code></pre>
<pre><code>## [1] 0.07234504</code></pre>
<p>Wir können den Wert aber auch viel einfacher direkt berechnen lassen und dass
für alle <span class="math inline">\(j\)</span> mit Hilfe von <code>cooks.distance(..)</code>:</p>
<pre class="r"><code>cooks.distance(erg_lm)[173]</code></pre>
<pre><code>##        173 
## 0.07234504</code></pre>
<p>Wann aber ist nun ein Wert ein <em>einflussreicher</em> Ausreißer?</p>
<p>Cook selber gibt dafür die Bedingung <span class="math inline">\(D_j &gt; 1\)</span> an. Andere Autor*innen schreiben <span class="math inline">\(D_j &gt; 4/n\)</span>, wobei <span class="math inline">\(n\)</span> die Anzahl der Beobachtung ist.</p>
<p>In unserem Beispiel liefert die Variante <span class="math inline">\(D_j &gt; 1\)</span></p>
<pre class="r"><code>cooks &lt;- cooks.distance(erg_lm)
names(cooks) &lt;- NULL
n &lt;- nrow(tips)

any(cooks &gt; 1)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>keinen Ausreißer.</p>
<p>Wenn wir jedoch mit <span class="math inline">\(D_j &gt; 4/n\)</span> suchen
.</p>
<pre class="r"><code>any(cooks &gt; 4/n)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>dann gibt es Ausreißer.
Die Indices dieser finden wir mit:</p>
<pre class="r"><code>which(cooks &gt; 4/n)</code></pre>
<pre><code>##  [1]  24  48  57 103 142 157 171 173 179 183 184 185 188 208 211 213 215 238</code></pre>
<p>Bereinigen wir nun unsere Daten um genau diese Werte:</p>
<pre class="r"><code>remove &lt;- which(cooks &gt; 4/n)
tips %&gt;% slice(-remove) -&gt; tips_no_outliers
erg_lm_no_outliers &lt;- lm(tip ~ total_bill, data = tips_no_outliers)</code></pre>
<p>Und schauen uns das Ergebnis an:</p>
<pre class="r"><code>summary(erg_lm_no_outliers)</code></pre>
<pre><code>## 
## Call:
## lm(formula = tip ~ total_bill, data = tips_no_outliers)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.22592 -0.48166 -0.06794  0.46992  2.31414 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.773324   0.139435   5.546  8.2e-08 ***
## total_bill  0.111799   0.006958  16.069  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7778 on 224 degrees of freedom
## Multiple R-squared:  0.5355, Adjusted R-squared:  0.5334 
## F-statistic: 258.2 on 1 and 224 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>gf_point(tip ~ total_bill, data = erg_lm_no_outliers) +
  geom_line(aes(x = total_bill, 
                y = erg_lm_no_outliers$fitted.values, 
                color = &quot;Regressionsgerade&quot;))</code></pre>
<p><img src="/post/2020-06-29-cook-abstand.de_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>Im direkten Vergleich:</p>
<pre class="r"><code>gf_point(tip ~ total_bill, data = erg_lm) +
  geom_line(aes(x = total_bill, 
                y = erg_lm$fitted.values, 
                colour = &quot;Regressionsgerade (Orginal)&quot;)) +
  geom_abline(aes( colour = &quot;Regressionsgerade (No Outliers)&quot;,
              intercept = erg_lm_no_outliers$coefficients[1], 
              slope =  erg_lm_no_outliers$coefficients[2])
              )</code></pre>
<p><img src="/post/2020-06-29-cook-abstand.de_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
